"""Created on Mon Feb 14 09:45:57 2022@author: Youssef"""# Import pandas libraryimport pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltfrom scipy import stats"""%matplotlib inlinecommand to plot on a jupyter notebook. """# Read the online file by the URL provides above, and assign it to variable "df"other_path = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv"df = pd.read_csv(other_path, header=None)df.head(3)headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",         "peak-rpm","city-mpg","highway-mpg","price"]#add column names/headersdf.columns = headersdf.head()# replace the "?" symbol with NaN# NaN/nan = Not a Number = Python's default missing value marker)# inplace=True arg allows to modify the input. default: inplace=False, retrun a# modified copy of the inputdf.replace("?", np.nan, inplace=True)# drop missing values (dropna()) along the column "price"df = df.dropna(subset=["price"], axis=0)  # specify axis to indicate by column# fill missing values with 0df[["price"]].fillna(0)# save csvdf.to_csv("path", index=False)  # without row indices"""Read/Save Other Data Formats:     csv	pd.read_csv()	df.to_csv()    json	pd.read_json()	df.to_json()    excel	pd.read_excel()	df.to_excel()    hdf	pd.read_hdf()	df.to_hdf()    sql	pd.read_sql()	df.to_sql()"""# know the data type of each column (str() in r)df.dtypes# get a statistical summary of each columndf.describe()df.info()# The default setting of "describe" skips variables of type object.# We can apply the method "describe" on the variables of type 'object':df.describe(include=["object"])# specific column summary; vec.describe(), for any vecdf["price"].describe()df[["price"]].describe()# cast each element in the column "price" to an integerdf["price"] = df["price"].astype("int")"""Data WranglingDef: Data wrangling is the process of converting data from the initial formatto a format that may be better for analysis."""# identify missing values (null means missing and not 0)df["price"].isnull()  # return a boolean vectordf["price"].notnull()  # opposite# number of missing values(df["price"].isnull()).value_counts()# Calculate the mean value for the "normalized-losses" columndf["normalized-losses"].astype("float").mean(axis=0)"""axis=0, axis=1 explanation:It specifies the axis along which the means are computed. By default axis=0.This is consistent with the numpy.mean usage when axis is specified explicitly(in numpy.mean, axis==None by default, which computes the mean value over theflattened array) , in which axis=0 along the rows (namely, index in pandas),and axis=1 along the columns."""# drop all rows with NaN in "price" columndf.dropna(subset=["price"], axis=0, inplace=True)# !after dropping rows, we must reset the df index/row numbersdf.reset_index(drop=True, inplace=True)# drop a columndf = df.drop('aspiration', axis = 1, inplace=True)# check the data types with attribute dtype and convert with astype()df[["price"]].dtypesdf[["price"]].astype("int")"""Data NormalizationNormalization is the process of transforming values of several variables intoa similar range. Typical normalizations include:     scaling the variable so the variable average is 0    scaling the variable so the variance is 1    or scaling the variable so the variable values range from 0 to 1.Most commun: replace original value by (original value)/(maximum value)."""# merge 2 dataframes to the original datafram (by row: axis=1)dummy_variable_2 = df[3:5]df = pd.concat([df, dummy_variable_2], axis=1)df.reset_index()"""Pandas selection by position:A. using []The following table shows return type values when indexing pandas objects with[] :    Object Type    Selection      Return       Series     series[label]  scalar value     DataFrame   frame[colname] Series corresponding to colnameB. using iloc attributeThe most robust and consistent way of slicing ranges along axes.use the attribut .iloc (for i localization) to access the cell as in numpy.Rk: to get df by colnames, use double brackets [[]]"""df[["make","price"]]df.iloc[3:5,]# rename columns: Use df.rename() function and refer the columns to be renamed.df.rename(columns = {"make":"brand", "price":"price_in_dollar"})# get z score of a vector: (x-mu)/sd(df["length"]-df["length"].mean())/df["length"].std()# get number of categories of a vectordf.columnsdf["horsepower-binned"].unique()# table() in R: pivot_table() in pandas# A pivot table has one variable displayed along the columns and the other variable displayed along the rows.pd.pivot_table(df)"""correlation scores: the correlation coefficient is ar its mximum when the scatterplot of thedataset are aligned.!= slope coefficient a.Define correlation as the linear association between two numerical variables:Use Pearson correlation as a measure of the correlation between two continuousvariables.variable x is a good predictior of variable y when there is a good correlation,i.g. points are aligned.when the regression line is close to horizontal/vertical, it means that thereis a high variability leading to the conclusion that x is a bad predictor of y.# Correlation and CausationCorrelation: a measure of the extent of interdependence between variables.Causation: the relationship between cause and effect between two variables.It is important to know the difference between these two.Correlation does not imply causation. Determining correlation is much simplerthe determining causation as causation may require independent experimentation.The Pearson Correlation measures the linear dependencebetween two variables X and Y.The resulting coefficient is a value between -1 and 1 inclusive, where:    1: Perfect positive linear correlation.    0: No l. correlation,the two variables most likely not affect each other.    -1: Perfect negative linear correlation.The P-value is the probability value that the correlation between thetwo variables is statistically significant.p-value< 0.001: strong evidence that the correlation is significant."""# compute correlation score between each pair of variables in a dataframe.# methods may be pearson, spearman, kendall, ...df.corr(method="pearson")# compute corr between specific columns/variables: select colums than, corr()df[['bore', 'stroke', 'compression-ratio', 'horsepower']].corr()# use "regplot" which plots the scatterplot plus the fitted regression line# for the data.sns.regplot(x="engine-size", y="price", data=df)plt.ylim(0, )# get unique values of a col/vecdf['drive-wheels'].unique()# count number of values in a df column. the output is of type Serie.df['drive-wheels'].value_counts()# covert a Serie to a dataframe: use to_frame()df['drive-wheels'].value_counts().to_frame()# group data in dataframe by category: use groupby()df.groupby(["make"], as_index=False)  # as_index: include row numbers or not# calculate the average price for each of the different categories of datadf.groupby(["make"]).mean()# calculate the Pearson Correlation Coefficient and P-valuepearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])"""ANOVA: Analysis of VarianceAnnova = to test whether there are significant differences between the meansof two or more groups. ANOVA returns two parameters:    - F-test score: ANOVA assumes the means of all groups are the same,    calculates how much the actual means deviate from the assumption, and    reports it as the F-test score.    A larger score means there is a larger difference between the means.    - P-value: P-value tells how statistically significant our calculated score    (F-test score) is.If our price variable is strongly correlated with the variablewe are analyzing, we expect ANOVA to return a sizeable F-test score anda small p-value."""